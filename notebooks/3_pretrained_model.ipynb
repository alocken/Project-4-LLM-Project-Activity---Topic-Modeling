{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 4: LLM Project Activity - Topic Modeling**\n",
        "### **Week 23** 3-Pre-Trained-Model"
      ],
      "metadata": {
        "id": "ETetHoc8bWCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select a pre-trained model for your project and perform data preprocessing. (9.4)\n",
        "\n",
        "- Project Task is Topic Modeling which is typically an unsupervised learning task and doesn't require labeled data\n",
        "- Commonly used methods could be LDA (classical approach) or BERTopic (using tranformer embeddings)\n",
        "- Noted through research that recommended approach to do topic modeling with transformers is using BERTopic\n",
        "- BERTopic cannot use Hugging Face pipeline() API\n",
        "- Although it's not a supported task type in Hugging Face's pipeline function, can utilize a pre-trained Hugging Face model in workflow\n",
        "- NLP Tasks performing is Topic Modeling including text preprocessing, text embedding, topic extraction, and inference\n",
        "- Hugging Face pre-trained model for specific task will be: guibvieira/topic_modelling\n"
      ],
      "metadata": {
        "id": "MZSKSq5Xtjl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load pre-trained model requirements: BERTopic\n",
        "!pip install -U bertopic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArPQWLV6BVJM",
        "outputId": "e4130f9e-3ce7-483c-ecd6-81dad385fa91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.8.40)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.1.0)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.67.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.8)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (24.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.53.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.33.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.14.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "#Load the embedding model (same one used when training)\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "#Load BERTopic model with embedding model passed in\n",
        "topic_model = BERTopic.load(\"guibvieira/topic_modelling\", embedding_model=embedding_model)\n",
        "\n",
        "#Extract tokenized documents\n",
        "docs_tokenized = ds_test[\"text\"].tolist()\n",
        "\n",
        "#Join tokens into strings\n",
        "docs = [\" \".join(tokens) for tokens in docs_tokenized]\n",
        "\n",
        "#Transform\n",
        "topics, probs = topic_model.transform(docs)\n",
        "\n",
        "#Show topic info\n",
        "print(topic_model.get_topic_info())\n",
        "\n",
        "#Print first few documents’ topics\n",
        "for i in range(5):\n",
        "    print(f\"Document {i} Topic: {topics[i]}, Probability: {probs[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642,
          "referenced_widgets": [
            "785bce037435449a92a707bed43fd063",
            "f6929a6791b44165b33824876442a506",
            "63fbeeb998fa425a89e50fe23867f428",
            "ded95a037d374b0bbb116b2adeba7b65",
            "7f2196e0359e404e983e0bb9d28a3fdc",
            "a8c646ee70984c84a66cab6c0a2042ea",
            "98631bce3e52453f92826adbc58892e7",
            "79a1dd9d812f4b4883d321a760b8d61c",
            "d55487e7715c4e29b9053e54ed9d8502",
            "08ead9e5ed65423a8c584a4222f00d6a",
            "20fa8bfbda9d4226b91c6074a31f9e8a"
          ]
        },
        "id": "71jCDGPBO0i8",
        "outputId": "08bd1d84-aa19-4930-edb3-7268b40c1042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/236 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "785bce037435449a92a707bed43fd063"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-04 07:11:32,931 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Topic   Count                                     Name  \\\n",
            "0        -1  241723             -1_liquidity_coins_bro_sorry   \n",
            "1         0    3993              0_token_tokens_supply_value   \n",
            "2         1    2185                   1_main_problem_bro_tho   \n",
            "3         2    2122           2_twitter_comment_space_social   \n",
            "4         3    2100  3_project_projects_interesting_research   \n",
            "...     ...     ...                                      ...   \n",
            "6281   6280      15         6280_listing_project_list_simple   \n",
            "6282   6281      15          6281_code_phone_number_withdraw   \n",
            "6283   6282      15                     6282_fun_details_dm_   \n",
            "6284   6283      15         6283_different_okay_wrong_people   \n",
            "6285   6284      15     6284_thought_bitcoin_members_service   \n",
            "\n",
            "                                         Representation  Representative_Docs  \n",
            "0     [liquidity, coins, bro, sorry, morning, answer...                  NaN  \n",
            "1     [token, tokens, supply, value, price, holders,...                  NaN  \n",
            "2     [main, problem, bro, tho, brother, form, solut...                  NaN  \n",
            "3     [twitter, comment, space, social, media, post,...                  NaN  \n",
            "4     [project, projects, interesting, research, mat...                  NaN  \n",
            "...                                                 ...                  ...  \n",
            "6281  [listing, project, list, simple, fine, team, a...                  NaN  \n",
            "6282  [code, phone, number, withdraw, wrong, correct...                  NaN  \n",
            "6283                   [fun, details, dm, , , , , , , ]                  NaN  \n",
            "6284  [different, okay, wrong, people, lol, sure, da...                  NaN  \n",
            "6285  [thought, bitcoin, members, service, people, d...                  NaN  \n",
            "\n",
            "[6286 rows x 5 columns]\n",
            "Document 0 Topic: 347, Probability: 0.382325142621994\n",
            "Document 1 Topic: 3907, Probability: 0.43404465913772583\n",
            "Document 2 Topic: 2453, Probability: 0.5277429223060608\n",
            "Document 3 Topic: 1482, Probability: 0.5384505987167358\n",
            "Document 4 Topic: 2543, Probability: 0.5540359020233154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Outputs**\n",
        "\n",
        "Number of topics found:\n",
        "- 6,286 topics (including topic -1, usually outlier or \"no topic\" group)\n",
        "\n",
        "Topic Distribution:\n",
        "- Topic -1 has the largest count with 241,723 documents assigned to it indicating these documents didn't fit well into any coherent topic\n",
        "- Other topics have much smaller document counts (e.g., topic 0 has 3,993 docs, topic 1 has 2,185 docs, etc.)\n",
        "- Distribution shows many very specific topics, some with as few as 15 documents\n",
        "\n",
        "Topic Names and Keywords:\n",
        "- Each topic's labeled with a numeric ID and name constructed from top representative words (e.g., 0_token_tokens_supply_value, 1_main_problem_bro_tho).\n",
        "- Keywords provide insight into the main theme or concept captured by each topic\n",
        "- Example keywords for topic 0 include: token, tokens, supply, value, price, holders — likely related to cryptocurrency tokens\n",
        "\n",
        "Topic Representations:\n",
        "- Each topic has a list of keywords (e.g., topic 3: twitter, comment, space, social, media, post), which summarize the topic's content.\n",
        "\n",
        "Document Topic Assignments for the first 5 documents:\n",
        "- Document 0 assigned to topic 347 with probability ~0.38\n",
        "- Document 1 assigned to topic 3907 with probability ~0.43\n",
        "- Document 2 assigned to topic 2453 with probability ~0.53\n",
        "- Document 3 assigned to topic 1482 with probability ~0.54\n",
        "- Document 4 assigned to topic 2543 with probability ~0.55\n",
        "\n",
        "Interpretation of Probabilities:\n",
        "- Indicate the confidence or strength of the topic assignment for each document.\n",
        "- Higher probability means the document fits well into that topic.\n",
        "\n",
        "Overall Interpretation\n",
        "- BERTopic model identified a very large number of detailed topics from dataset.\n",
        "- Most documents are assigned to an outlier topic -1, possibly indicating many documents don't strongly fit well into the discovered topics or topics could be pruned for better clarity.\n",
        "- Named topics provide useful keywords summarizing major themes, useful for understanding your corpus' thematic structure\n",
        "- Model assigns each document to a topic with a probability score, indicating how confidently the document matches that topic."
      ],
      "metadata": {
        "id": "OTE9A32xS5NR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rerun with additional preprocessing implementing lowercase, removing punctuation and stopwords\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "#Download NLTK stopwords (only run once)\n",
        "download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "#Define text cleaning function\n",
        "def clean_tokens(tokens):\n",
        "    # Lowercase, remove punctuation and stopwords\n",
        "    cleaned = [re.sub(r\"\\W+\", \"\", word.lower()) for word in tokens]\n",
        "    return [word for word in cleaned if word and word not in stop_words]\n",
        "\n",
        "#Load embedding model and BERTopic model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "topic_model = BERTopic.load(\"guibvieira/topic_modelling\", embedding_model=embedding_model)\n",
        "\n",
        "#Extract and preprocess your tokenized dataset\n",
        "docs_tokenized = ds_test[\"text\"].tolist()  # Assuming each entry is a list of tokens\n",
        "docs_cleaned = [\" \".join(clean_tokens(tokens)) for tokens in docs_tokenized]\n",
        "\n",
        "#Transform using BERTopic\n",
        "topics, probs = topic_model.transform(docs_cleaned)\n",
        "\n",
        "#View topic summary\n",
        "print(topic_model.get_topic_info())\n",
        "\n",
        "#Show topic assignments for a few example docs\n",
        "for i in range(5):\n",
        "    print(f\"Document {i} → Topic: {topics[i]} | Probability: {probs[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677,
          "referenced_widgets": [
            "50111467c1884189b64f9c9afb0f9ca2",
            "5e99f724cccc4b4197b8473df9231f2d",
            "abaaba1bed154326a0578d5b46cd7375",
            "366bf7f7a06b46a0a731bcbe5d3fc8a0",
            "07b1ec0f13a742e1838b1c8581b37653",
            "86323945630a43faabc174d5f8759c04",
            "65b8f365a20d459c81310d8ee649ca56",
            "0324c009ac164e0982ffba95a011fd98",
            "db6d13e91b3a4dfba9d890eeeb46b9b7",
            "534b5627a7b34c519d92f211319b7f0d",
            "0f6b3f06265d4ff6bfb1cd197a56fefc"
          ]
        },
        "id": "dQu1m2IsbOIv",
        "outputId": "d0a85357-f175-43f6-b169-9b6bd76007ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/236 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50111467c1884189b64f9c9afb0f9ca2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-04 07:54:23,767 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Topic   Count                                     Name  \\\n",
            "0        -1  241723             -1_liquidity_coins_bro_sorry   \n",
            "1         0    3993              0_token_tokens_supply_value   \n",
            "2         1    2185                   1_main_problem_bro_tho   \n",
            "3         2    2122           2_twitter_comment_space_social   \n",
            "4         3    2100  3_project_projects_interesting_research   \n",
            "...     ...     ...                                      ...   \n",
            "6281   6280      15         6280_listing_project_list_simple   \n",
            "6282   6281      15          6281_code_phone_number_withdraw   \n",
            "6283   6282      15                     6282_fun_details_dm_   \n",
            "6284   6283      15         6283_different_okay_wrong_people   \n",
            "6285   6284      15     6284_thought_bitcoin_members_service   \n",
            "\n",
            "                                         Representation  Representative_Docs  \n",
            "0     [liquidity, coins, bro, sorry, morning, answer...                  NaN  \n",
            "1     [token, tokens, supply, value, price, holders,...                  NaN  \n",
            "2     [main, problem, bro, tho, brother, form, solut...                  NaN  \n",
            "3     [twitter, comment, space, social, media, post,...                  NaN  \n",
            "4     [project, projects, interesting, research, mat...                  NaN  \n",
            "...                                                 ...                  ...  \n",
            "6281  [listing, project, list, simple, fine, team, a...                  NaN  \n",
            "6282  [code, phone, number, withdraw, wrong, correct...                  NaN  \n",
            "6283                   [fun, details, dm, , , , , , , ]                  NaN  \n",
            "6284  [different, okay, wrong, people, lol, sure, da...                  NaN  \n",
            "6285  [thought, bitcoin, members, service, people, d...                  NaN  \n",
            "\n",
            "[6286 rows x 5 columns]\n",
            "Document 0 → Topic: 347 | Probability: 0.382325142621994\n",
            "Document 1 → Topic: 3907 | Probability: 0.43404465913772583\n",
            "Document 2 → Topic: 2453 | Probability: 0.5277429223060608\n",
            "Document 3 → Topic: 1482 | Probability: 0.5384505987167358\n",
            "Document 4 → Topic: 2543 | Probability: 0.5540359020233154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Observation:** outputs from further preprocessed data remain unchanged from intial. This is likely due to BERTopic embedding models (SentenceTransformers), inference uses fixed topics, consistent input, stable embeddings."
      ],
      "metadata": {
        "id": "2rGMJeP3b1e2"
      }
    }
  ]
}
