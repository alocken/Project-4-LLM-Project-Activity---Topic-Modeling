{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 4: LLM Project Activity - Topic Modeling**\n",
        "### **Week 23** 4-Optimization\n",
        "Apply transfer learning concepts to enhance the model, followed by evaluating and optimizing the project model and creating an LLM Model Report. (9.5)\n",
        "\n",
        "**First Step:** Transfer Learning Concepts\n",
        "- Continue work with the dataset, next step is to fine-tune the model by first loading all the required packages to support loading model, preparing dataset, training model, and evaluating performance\n",
        "- From Hugging Face Tasks Guides for Natural Language Processing (NLP), selected \"Text Classification\" as this is a common BERTopic embeddings that will align with the next goal of a predictive model, specifically supervised prediction of topics on unseen documents).\n",
        "- Output of this process will be a classifier that assigns the most likely topic to new text (instead of re-running BERTopic)\n",
        "\n",
        "Additional information about tokenizer, classification model, data, and task:\n",
        "- Tokenizer: sentence-transformers/all-MiniLM-L6-v2\n",
        "- Classification model: microsoft/MiniLM-L12-H384-uncased (MiniLM architecture with classification head) - this is a lightweight transformer-based model pre-trained for general language understanding tasks and 'with classification head' indicates model equipped with classification layer to perform classification tasks, including text classification.\n",
        "- Data: Preprocessed and tokenized dataset from BERTopic labels\n",
        "- Task: Text classification (with accuracy and F1 metrics)\n",
        "\n"
      ],
      "metadata": {
        "id": "BjVwy8x4U5YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save model; mount to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhvA5ZJcMdU_",
        "outputId": "bf88d42e-cb5e-4a5f-a010-40778327a99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install required packages\n",
        "!pip install transformers datasets torch accelerate evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTr4wFMNQ545",
        "outputId": "dcd5900a-ef72-4698-e71e-4dc31cdcaec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set up and imports\n",
        "import os\n",
        "from datasets import Dataset, DatasetDict, load_dataset, ClassLabel\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import evaluate\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")"
      ],
      "metadata": {
        "id": "Pc4fGZiFMv-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare data\n",
        "df = pd.DataFrame({\n",
        "    \"text\": docs_cleaned,            # Preprocessed text strings\n",
        "    \"topic\": topics                  # BERTopic-generated labels\n",
        "})\n",
        "\n",
        "# Remove rare topics\n",
        "counts = df[\"topic\"].value_counts()\n",
        "df = df[df[\"topic\"].isin(counts[counts>=2].index)]\n",
        "\n",
        "# Map topic IDs to 0…N-1\n",
        "topic_to_label = {t:i for i,t in enumerate(sorted(df[\"topic\"].unique()))}\n",
        "df[\"label\"] = df[\"topic\"].map(topic_to_label)\n",
        "\n",
        "# Stratified train/validation split\n",
        "df_train, df_val = train_test_split(df, stratify=df[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(df_train[[\"text\",\"label\"]].reset_index(drop=True)),\n",
        "    \"validation\": Dataset.from_pandas(df_val[[\"text\",\"label\"]].reset_index(drop=True))\n",
        "})"
      ],
      "metadata": {
        "id": "PZ660gzsMv2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize and format\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def preprocess(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "encoded = dataset.map(preprocess, batched=True)\n",
        "encoded = encoded.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Specify label feature for correct dtype\n",
        "num_labels = len(topic_to_label)\n",
        "encoded = encoded.cast_column(\"labels\", ClassLabel(num_classes=num_labels))\n",
        "encoded.set_format(\"torch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "5150780877c346d1a980e1f71c283440",
            "6525ed780de445db8bd06941aea8698d",
            "81519d1392a84ab18484c3369b88aaff",
            "bf2344f0354b44debea5692eba714169",
            "49dd2f6a7a804b6395a3b60136382928",
            "b627386219794713aaff29e70780e2c2",
            "11a74fb3229c498d833dbc2ce87e6a2d",
            "4f0e3944621a44068c01462b1e391cda",
            "ef0f9df338884ababd5a484adf5a0c98",
            "c37c17127ed94c2da6d7a0b3d81645e9",
            "904c63f51c6447e3a0e9a1de565e6d91",
            "bf79fa11ab444effa6c23803e7c821b3",
            "20eb11347e8448c08f09ab16297b78aa",
            "aa4dac92a491474c9111d34dd4c5be93",
            "45d4bcdd5b604f67a5232129aa0bd1a4",
            "91d2cb30bf5e48c3855faad9e4e20061",
            "d47f071c21734d0bb2ea20dd9eafddd4",
            "10c07200c8c346dca16397fe1d55cf0a",
            "b6519cf3624d4d65ba486eedfe2cf9dc",
            "78e6274bc4b14d4aa59e7268fb92563a",
            "3540f5960a3449289f8f130899699b7c",
            "258976fff3b6470d9e589f92d575d192",
            "061b8a08ac174b3cab6e6adc71ef0b83",
            "c0b11094beaa480a9b0625b168f1264e",
            "303e0bc9b4bd4fa69b908ebf7b4ffce7",
            "3061a18198f94b468c22013d57bc83a4",
            "076e0083357b4c8aa9de7a7f3b78e5c2",
            "5c88e57286ac45c381eb1a80bb21ff20",
            "75c97e5447b348f0bc6609f837bd380b",
            "a51ff0754cad43919af16696f7d37623",
            "8bc6bb0a49ac4a94b25419b66184cdda",
            "2ebfbe69cd194080a2363888211af56a",
            "e8efae713afa4c188e013991e15bf41d",
            "4f5015df8e724c69bcbbb7f377f896be",
            "28f7cba35387439c86ed2e8d32416962",
            "5be80f156b434f99bbe829746235fb09",
            "9627b1fc4330437393fa547db421f29d",
            "6d88805a5c7c4be89b5a10611740b0fa",
            "430cc4676f2e427bbe0eb2e87c88efca",
            "d0a650765e2349e4a7320cdfb3686c82",
            "27d3f49b5f4a4970aa187f853d4c9fba",
            "5bb5ce1009a642b6915b7b8b7d3ca663",
            "d23e97f8b58a43eab7dcb721d9ae71a0",
            "dc37aaaba61545509623d49c1ef26368"
          ]
        },
        "id": "zKdWN11JMvuI",
        "outputId": "5be2a2d6-6755-47a0-b1dc-0a45fa86ad56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5519 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5150780877c346d1a980e1f71c283440"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1380 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf79fa11ab444effa6c23803e7c821b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Casting the dataset:   0%|          | 0/5519 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "061b8a08ac174b3cab6e6adc71ef0b83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Casting the dataset:   0%|          | 0/1380 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f5015df8e724c69bcbbb7f377f896be"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1w = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"f1_weighted\": f1w.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "EhsGdGmNM_sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and fine tune model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/MiniLM-L12-H384-uncased\",\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded[\"train\"],\n",
        "    eval_dataset=encoded[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wcxv-sZANF5f",
        "outputId": "0d2be9bb-94bb-42de-ab3d-a08694b9fc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malia-locken\u001b[0m (\u001b[33malia-locken-lighthouse-labs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250704_213206-p7u0o71j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alia-locken-lighthouse-labs/huggingface/runs/p7u0o71j' target=\"_blank\">my_model</a></strong> to <a href='https://wandb.ai/alia-locken-lighthouse-labs/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alia-locken-lighthouse-labs/huggingface' target=\"_blank\">https://wandb.ai/alia-locken-lighthouse-labs/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alia-locken-lighthouse-labs/huggingface/runs/p7u0o71j' target=\"_blank\">https://wandb.ai/alia-locken-lighthouse-labs/huggingface/runs/p7u0o71j</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1035' max='1035' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1035/1035 03:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>6.708900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>6.711000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>6.701700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>6.694400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>6.686800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>6.660700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>6.636400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>6.622100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>6.635900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>6.625400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>6.616500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>6.608000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>6.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>6.579700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>6.511500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>6.511200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>6.517400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>6.558300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>6.479100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>6.504400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>6.426100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>6.475100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>6.408000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>6.382900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>6.392300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>6.359200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>6.420000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>6.420900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>6.501500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>6.438000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>6.369900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>6.397200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>6.384400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>6.315800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>6.316700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>6.270400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>6.301900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>6.282600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>6.277000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>6.246000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>6.384200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>6.239200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>6.301600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>6.215400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>6.156900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>6.428200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>6.197900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>6.245500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>6.215400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>6.269400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>6.216200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>6.203300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>6.136600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>6.205000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>6.091100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>6.241900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>6.227100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>6.211400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>6.115400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>6.207300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>6.104400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>6.198400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>6.128100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>6.107800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>5.983100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>6.300200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>6.068700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>6.168700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>6.059500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>6.038000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>6.072400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>5.993700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>6.189500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>6.112500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>6.106700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>6.044900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>6.141600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>5.992700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>6.157800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>6.131200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>6.045900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>6.120000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>6.147400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>6.164000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>6.144800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>6.110400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>6.096000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>6.016700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>6.016300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>6.127000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>6.136800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>6.056000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>6.343600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>6.077000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>6.117500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>6.049100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>6.018800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>5.920700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>5.901700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.961300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>6.009900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>5.989400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>6.090200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1035, training_loss=6.267840712542695, metrics={'train_runtime': 208.8339, 'train_samples_per_second': 79.283, 'train_steps_per_second': 4.956, 'total_flos': 276678068599296.0, 'train_loss': 6.267840712542695, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "AaU4oWgooOz8",
        "outputId": "68276c4f-5ef3-469c-d1a5-c0990b101050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='87' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [87/87 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 6.021093845367432,\n",
              " 'eval_accuracy': 0.08333333333333333,\n",
              " 'eval_f1_weighted': 0.031218083104875557,\n",
              " 'eval_runtime': 3.1587,\n",
              " 'eval_samples_per_second': 436.894,\n",
              " 'eval_steps_per_second': 27.543,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Evaluation Summary**\n",
        "\n",
        "The model was trained for 3 epochs, completing a total of 1,035 steps. The training loss was approximately 6.27, which is relatively high and indicates that the model struggled to learn meaningful representations from the training data. In terms of speed, training processed about 79 samples per second and approximately 5 steps per second, this performance is typical and reflects hardware capabilities rather than learning quality.\n",
        "\n",
        "On evaluation, the model produced an evaluation loss of ~6.02, very close to the training loss. This suggests the model did not overfit, but also failed to learn effectively. The accuracy is extremely low at 8.3%, which, given a high number of topic classes (as often produced by BERTopic), may be near random guessing. The weighted F1 score of ~0.03 further supports this, indicating the model cannot meaningfully differentiate between classes, especially in the presence of class imbalance.\n",
        "\n",
        "Several contributing factors may explain the poor performance. First, a large number of topic classes (e.g., 800+) creates a challenging classification task with limited training examples per class. Reducing or merging infrequent topics may help. Second, label imbalance can lead to poor generalization and addressing this with better stratification or data weighting may be necessary. Third, suboptimal hyperparameters or model limitations (MiniLM being a lightweight model) may hinder learning, in which case adjusting learning rates or switching to a more expressive architecture like BERT-base could improve results. Additionally, the unsupervised nature of BERTopic label generation may introduce label noise, so some manual curation or semi-supervised refinement could enhance label quality. Overall, while the model is functioning, it is not currently learning effectively and will require targeted improvements to become viable."
      ],
      "metadata": {
        "id": "VI6hzNckZ0zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save and inference\n",
        "trainer.save_model(\"my_model\")\n",
        "tokenizer.save_pretrained(\"my_model\")\n",
        "\n",
        "#Pipeline usage for inference\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"text-classification\", model=\"my_model\", tokenizer=\"my_model\")\n",
        "print(classifier(\"Here is a new unseen document...\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptFbx6E-NP8_",
        "outputId": "9f918b1f-c86a-432f-c9b8-bf5945e2c0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'LABEL_23', 'score': 0.02989175170660019}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save final trained model to Google Drive\n",
        "model_save_path = \"/content/drive/MyDrive/topic_modeling_project4\"\n",
        "\n",
        "#Save the model, tokenizer, and config\n",
        "trainer.save_model(model_save_path)        #Save model, config, and training state\n",
        "tokenizer.save_pretrained(model_save_path) #Save tokenizer files too"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AomPvbbtoOlR",
        "outputId": "28353316-b5f7-4a72-e400-adb7c280862a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/topic_modeling_project4/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/topic_modeling_project4/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/topic_modeling_project4/vocab.txt',\n",
              " '/content/drive/MyDrive/topic_modeling_project4/added_tokens.json',\n",
              " '/content/drive/MyDrive/topic_modeling_project4/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Step: Last Project Task**\n",
        "\n",
        "Evaluate and Optimize model:\n",
        "- Optimize performance\n",
        "- Implement findings from the optimization task (Optimizing LLM Performance)\n",
        "- Retrain model using changes to hyperparameters\n",
        "- Bonus: fine-tune multiple models for a point of comparison"
      ],
      "metadata": {
        "id": "lz0jRrW2ssR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Authenticate notebook and send model to hub\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "1155f9d70519467bb691fee42c05b7fe",
            "581f9978a5174ffea3ae69f30e8c387c",
            "fa962a58672344feb4f37a2668dab819",
            "1e9a29ab26154b83b40bb11055f0627d",
            "2e7ebdb72784484494dfd9e37eedda39",
            "ef4ed866c8884246ba62bf7d0ffde941",
            "3e03fce9ea3844f193e6c426789e6f1d",
            "d09dd5a7dddd4db4951dacd6bcba510b",
            "4ec2a0bb556e4d818b0c6af4eaca69bf",
            "d349ea7b4345419f83483117df0d1d16",
            "d5d4045620b0412a9a067f8009b776a9",
            "0221bbd245ec4ecc8a3d3216fa44ee39",
            "5973b09409c44b8abe26930c5a6c5b18",
            "d8df9bc9fda645879faeffecc2bdea3d",
            "da137871d3cf4301804d373caaf46021",
            "9b60d00517354e2baa2fa3dc3562905f",
            "02b45a10f518415d97daea1fa0c72e29",
            "3e8a43ff1ae44a698da2bfbcbe57af68",
            "b9186509befd4a8ba71d732c2ee23343",
            "11c9772ee2ee4f8cbaafb8dd0a4b3900"
          ]
        },
        "id": "T3Yro2eFnOtI",
        "outputId": "2cbef143-3a96-467a-954f-2d01705fd688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1155f9d70519467bb691fee42c05b7fe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git-lfs #Size of model and assets\n",
        "!pip install -q transformers datasets evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLPaWzxOt0U3",
        "outputId": "e782423f-da96-41fc-fa92-a6587fc58707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_name = \"topic_modeling_project4\"  #model repo name"
      ],
      "metadata": {
        "id": "GmLLDyYYt0SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDPc9usbUbz8",
        "outputId": "ceb5e747-f0d7-4889-e0a5-43d09aeec8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model #1: adjusted hyperparameters to attempt to improve model performance\n",
        "#Repeat all steps to train model, implement changes to Training Arguments to pass to Trainer object\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=repo_name,\n",
        "    push_to_hub=True,\n",
        "    learning_rate=1e-5,  #Lowered to prevent overshooting minima\n",
        "    per_device_train_batch_size=8,  #Reduced batch for better generalization\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=6,  # Added more epochs to allow learning with a harder task\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    metric_for_best_model=\"f1_weighted\",\n",
        "    greater_is_better=True\n",
        ")\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded[\"train\"],\n",
        "    eval_dataset=encoded[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "iviXmt8Xt0QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Review model performance\n",
        "trainer.evaluate()\n",
        "\n",
        "#Evaluate on the validation set and print metrics\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "wPYVpHOUt0Nw",
        "outputId": "b38320bb-b82b-4f27-b6b3-702b931b3a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='346' max='173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [173/173 00:13]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 6.021093845367432, 'eval_model_preparation_time': 0.0051, 'eval_accuracy': 0.08333333333333333, 'eval_f1_weighted': 0.031218083104875557, 'eval_runtime': 5.8309, 'eval_samples_per_second': 236.671, 'eval_steps_per_second': 29.67}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model #1 Outputs Summary:**\n",
        "The evaluation of Model #1 shows that its performance remained essentially unchanged from the previous run. The evaluation loss was 6.02, identical to the earlier result, indicating no improvement in optimization or generalization. Accuracy was again just 8.3%, which is extremely low and likely near random guessing given the large number of topic classes typically generated by BERTopic. The weighted F1 score remained at approximately 0.0312, further confirming that the model is not effectively learning to differentiate between classes. While the evaluation speed improved slightly to around 237 samples per second, this is tied to hardware and does not reflect better learning. Overall, this model did not perform any better than the first attempt. The lack of improvement despite increasing training epochs suggests that the model is not learning, likely due to challenges such as a high number of classes, label imbalance, noisy topic labels from BERTopic, and the limitations of using a lightweight architecture like MiniLM for a complex classification task. Hyperparameters and label quality will likely require significant refinement for any meaningful performance gains."
      ],
      "metadata": {
        "id": "ahpQpEHkvMci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model #2: due to significance of underperformance observed in Model #1, exploring integration of class reduction into pipeline along with adjusted hyperparameters to determin if improvement in performance\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "\n",
        "#Build DataFrame from preprocessed docs and BERTopic labels\n",
        "df = pd.DataFrame({\n",
        "    \"text\": docs_cleaned,     #list of cleaned text strings\n",
        "    \"topic\": topics           #BERTopic-generated topic IDs\n",
        "})\n",
        "\n",
        "#Remove rare topics (class reduction)\n",
        "topic_counts = Counter(df[\"topic\"])\n",
        "allowed = {t for t, c in topic_counts.items() if c >= 10}\n",
        "df = df[df[\"topic\"].isin(allowed)]\n",
        "\n",
        "#Remap topics to continuous label indices\n",
        "label_map = {t: i for i, t in enumerate(sorted(df[\"topic\"].unique()))}\n",
        "df[\"label\"] = df[\"topic\"].map(label_map)\n",
        "num_labels = len(label_map)\n",
        "\n",
        "#Stratified train/validation split\n",
        "df_train, df_val = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
        "\n",
        "#Convert to Hugging Face Dataset format\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(df_train[[\"text\", \"label\"]].reset_index(drop=True)),\n",
        "    \"validation\": Dataset.from_pandas(df_val[[\"text\", \"label\"]].reset_index(drop=True)),\n",
        "})\n",
        "\n",
        "#Tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "def preprocess(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "encoded = dataset.map(preprocess, batched=True).rename_column(\"label\", \"labels\")\n",
        "encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "#Metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1w = evaluate.load(\"f1\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"f1_weighted\": f1w.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
        "    }\n",
        "\n",
        "#Load classification model aligned with tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",  #stronger reduced classes\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "#TrainingArguments with improved hyperparameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=repo_name,\n",
        "    push_to_hub=True,\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=6,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    max_grad_norm=1.0,\n",
        "    metric_for_best_model=\"f1_weighted\",\n",
        "    greater_is_better=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "#Train and evaluate\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded[\"train\"],\n",
        "    eval_dataset=encoded[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "67854cb94f25427487bdf9380500c58f",
            "bbbc2e9207f5432e8e337d39c4573bde",
            "823b467f8ea94fe2ab88119fe6fd261e",
            "bc0bb5e59c9f4a41ba978a71fe540552",
            "d2ed5ff5d7b347719337473d95ff19db",
            "61dd3c2c5767416a991565f83c123109",
            "efb65c81f2d7428fb8307a371f660c63",
            "e19eed54f5334b848773fa329260825b",
            "80ea05028d01457d93cac42cbd8887d6",
            "d1af83c4d171430a91234081373378ca",
            "273c26ea08ea46f3b9f0b369762f0e98",
            "8284bb017c0d4bb7b74371bcba5451d2",
            "d76f84c424d54fd18fdf995d9cc9db54",
            "9abe10a42ee84a75b126fe877b474117",
            "937c524437424a0189d5cbcece7bff2e",
            "0347021725574b14ae6d6f50130c6015",
            "c9143993992b4eb2a310753e33904b93",
            "2b8dc57681d241e2b15f29442ed2f4fa",
            "f5126b563093440094a896a197abad45",
            "83fea59b0b574369afc5119c704374e2",
            "f3bca90d1d504c0b9140090dadc9b11b",
            "fa6f6299f49944249c47e28d66d24b56",
            "21bea298ff5b47e6ae6f5e3bf11a00b1",
            "a4d7050689134301b49d70cabc9293da",
            "a95f0d84ad424490b2d3ea26c3921033",
            "eeb622d98b514be485b8fc45b550cd32",
            "56e8bb3498cf439ea7f0cf494ea2c831",
            "b4d350b7949e44ac936b40eb7c0a7dcc",
            "667cccbb5ef44e4388e96223f224ba21",
            "19babb4e22d14fc0b4bbe4a5368075c0",
            "f7ecff0f8b57459ea927d569a96b145d",
            "cfdb3524449b4057bf6dbca7f36fc2d3",
            "b6f5742de39d4c14bb474ee5a9ece3b0",
            "f8f82daae5b14a40a65f1e0e22ac8502",
            "d6e328905f4a4a879d223689aa63308f",
            "66fbe5cb8ced4296bee92644ba6a4c0e",
            "37abc95305f94a4893bdb5ba8a276913",
            "847975b22c424205aa54a51ca9406793",
            "2c1d88c55e9246bf9c64eb6d7c540347",
            "66e956fffd3d46c1b8f33d8af6731b72",
            "05b13b3ca04041408f95b19cfe74d5f0",
            "6d612fb953b24890b6326118669a687d",
            "8b36c8f050514ffe8f54eb79eeb46b5c",
            "1791ff16cf03471d826d652bfa94ded9"
          ]
        },
        "id": "MaRBdsI9dFfh",
        "outputId": "4e9da7b6-70ae-479a-f5cc-e3a5d92c35de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3531 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67854cb94f25427487bdf9380500c58f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/883 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8284bb017c0d4bb7b74371bcba5451d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21bea298ff5b47e6ae6f5e3bf11a00b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8f82daae5b14a40a65f1e0e22ac8502"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2652' max='2652' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2652/2652 11:51, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.160900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.115900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>5.057000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.077900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>5.007500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.035700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>5.073500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>4.992900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>5.025300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.949000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>4.977300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>4.893200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>4.847100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>4.955900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>4.826700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>4.715200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>4.785700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>4.612900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>4.500600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>4.811600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>4.436100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>4.639200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>4.494100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>4.474200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>4.605500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>4.332500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>4.419700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>4.116000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>4.291900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>4.308900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>4.297100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>4.552600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>4.232900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>4.369500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>4.296900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>4.071400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>4.273700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>4.178700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>4.094100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>4.189200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>4.476600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>4.084100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>3.971300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>4.129800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.822100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>3.881700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>3.975400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>3.917900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>3.801900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.647600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>3.653900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>4.009800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>3.946400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>3.751700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>3.837600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>3.726300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>3.604700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>3.825400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>3.831500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.753700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>3.801600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>3.748000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>3.752500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>3.891600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>3.531400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>3.439300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>4.065000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>3.576500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>3.612200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>3.293000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>3.454900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>4.125500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>3.597800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>3.438400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>3.805300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>3.395700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>4.094100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>3.370500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>3.615300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>3.514400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>3.825900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>3.529700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>3.580400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>3.728600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>3.363800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>3.757000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>3.486600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>3.367300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>3.542000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>3.123900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>3.432800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>3.288800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>3.313800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>3.592300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>3.492900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>3.282600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>3.349300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>3.715800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>3.687800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.038700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>3.112700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>3.180500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>3.129800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>3.010900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>3.106400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>3.013500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>3.427100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>3.549000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>3.539400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>3.136800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>3.292400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>3.196000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>3.072400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>3.413800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>3.035500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>3.265200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>3.293900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>3.491600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>3.164900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>3.396200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>3.142900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>3.353200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>2.920900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>3.256100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>3.492100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>3.366600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>3.303900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>3.265800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>3.132500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>3.165200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>3.146100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>3.101600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>3.203100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>3.199300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>3.132500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>3.273100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1370</td>\n",
              "      <td>3.503700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>3.251000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1390</td>\n",
              "      <td>3.272400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>3.223300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1410</td>\n",
              "      <td>3.047400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1420</td>\n",
              "      <td>2.880500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1430</td>\n",
              "      <td>3.149100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1440</td>\n",
              "      <td>3.085800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>2.856700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1460</td>\n",
              "      <td>3.421600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1470</td>\n",
              "      <td>3.068100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1480</td>\n",
              "      <td>2.820000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1490</td>\n",
              "      <td>2.905400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.909700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1510</td>\n",
              "      <td>2.907700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1520</td>\n",
              "      <td>3.049700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1530</td>\n",
              "      <td>2.706400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1540</td>\n",
              "      <td>3.008700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>3.023400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1560</td>\n",
              "      <td>2.682500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1570</td>\n",
              "      <td>3.193800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1580</td>\n",
              "      <td>3.264500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1590</td>\n",
              "      <td>3.602800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>3.130900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>3.076600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1620</td>\n",
              "      <td>2.832900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1630</td>\n",
              "      <td>2.818300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1640</td>\n",
              "      <td>2.952600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>2.739200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1660</td>\n",
              "      <td>3.084700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1670</td>\n",
              "      <td>3.188600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1680</td>\n",
              "      <td>3.116100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1690</td>\n",
              "      <td>2.571900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>2.729100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1710</td>\n",
              "      <td>2.764100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1720</td>\n",
              "      <td>3.271500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1730</td>\n",
              "      <td>2.755600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1740</td>\n",
              "      <td>2.828000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>2.992200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1760</td>\n",
              "      <td>2.858900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1770</td>\n",
              "      <td>3.009300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1780</td>\n",
              "      <td>2.619000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1790</td>\n",
              "      <td>2.411600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>2.926300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1810</td>\n",
              "      <td>2.895800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1820</td>\n",
              "      <td>2.923100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1830</td>\n",
              "      <td>2.736200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1840</td>\n",
              "      <td>2.751400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>2.787700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1860</td>\n",
              "      <td>3.010100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1870</td>\n",
              "      <td>2.729000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1880</td>\n",
              "      <td>3.033000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1890</td>\n",
              "      <td>2.948400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>2.733000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1910</td>\n",
              "      <td>3.140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1920</td>\n",
              "      <td>3.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1930</td>\n",
              "      <td>2.676400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1940</td>\n",
              "      <td>2.664000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>2.967800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1960</td>\n",
              "      <td>2.823000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1970</td>\n",
              "      <td>3.066000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1980</td>\n",
              "      <td>3.018400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1990</td>\n",
              "      <td>2.676300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.795400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2010</td>\n",
              "      <td>2.964500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2020</td>\n",
              "      <td>2.819600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2030</td>\n",
              "      <td>2.515600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2040</td>\n",
              "      <td>3.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>2.780300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2060</td>\n",
              "      <td>3.027800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2070</td>\n",
              "      <td>2.929000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2080</td>\n",
              "      <td>3.146400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2090</td>\n",
              "      <td>2.978100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>3.024400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2110</td>\n",
              "      <td>2.667100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2120</td>\n",
              "      <td>2.666500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2130</td>\n",
              "      <td>2.752200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2140</td>\n",
              "      <td>3.135700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>2.790700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2160</td>\n",
              "      <td>3.099700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2170</td>\n",
              "      <td>2.724600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2180</td>\n",
              "      <td>2.663500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2190</td>\n",
              "      <td>3.102400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>2.812700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2210</td>\n",
              "      <td>2.720400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2220</td>\n",
              "      <td>2.969800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2230</td>\n",
              "      <td>2.467900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2240</td>\n",
              "      <td>2.825400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>2.719600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2260</td>\n",
              "      <td>2.840500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2270</td>\n",
              "      <td>2.796800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2280</td>\n",
              "      <td>2.770400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2290</td>\n",
              "      <td>2.847700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>2.790700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2310</td>\n",
              "      <td>2.755700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2320</td>\n",
              "      <td>2.986600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2330</td>\n",
              "      <td>2.968800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2340</td>\n",
              "      <td>2.548200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>2.867600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2360</td>\n",
              "      <td>2.753300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2370</td>\n",
              "      <td>2.646600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2380</td>\n",
              "      <td>3.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2390</td>\n",
              "      <td>2.789200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>2.896200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2410</td>\n",
              "      <td>2.999900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2420</td>\n",
              "      <td>2.671300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2430</td>\n",
              "      <td>2.668200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2440</td>\n",
              "      <td>2.842500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>2.566800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2460</td>\n",
              "      <td>2.541900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2470</td>\n",
              "      <td>2.739200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2480</td>\n",
              "      <td>2.865100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2490</td>\n",
              "      <td>2.581500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>2.904000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2510</td>\n",
              "      <td>2.736000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2520</td>\n",
              "      <td>2.507000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2530</td>\n",
              "      <td>2.725100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2540</td>\n",
              "      <td>2.642600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>2.839300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2560</td>\n",
              "      <td>2.475600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2570</td>\n",
              "      <td>2.941900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2580</td>\n",
              "      <td>2.896300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2590</td>\n",
              "      <td>2.825800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>3.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2610</td>\n",
              "      <td>2.898700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2620</td>\n",
              "      <td>2.614900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2630</td>\n",
              "      <td>2.750500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2640</td>\n",
              "      <td>2.668200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>2.611500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [111/111 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 2.913451910018921, 'eval_accuracy': 0.42242355605889015, 'eval_f1_weighted': 0.313321903378084, 'eval_runtime': 6.379, 'eval_samples_per_second': 138.423, 'eval_steps_per_second': 17.401, 'epoch': 6.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Review model performance\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "tqlNwEm0XKUH",
        "outputId": "97879c91-b7f7-4c07-85f7-5dfeaf2db6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='222' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [111/111 00:19]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 2.913451910018921,\n",
              " 'eval_accuracy': 0.42242355605889015,\n",
              " 'eval_f1_weighted': 0.313321903378084,\n",
              " 'eval_runtime': 6.5197,\n",
              " 'eval_samples_per_second': 135.435,\n",
              " 'eval_steps_per_second': 17.025,\n",
              " 'epoch': 6.0}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model #2 Outputs Summary:**\n",
        "Building upon the initial hyperparameter tuning in Model #1, Model #2 introduced key enhancements to address the poor performance observed earlier. The pipeline was updated to reduce number of classes by filtering out rare topics with fewer than 10 examples, reducing noise and data sparsity. Topic IDs were remapped into compact, continuous label indices to facilitate classification. The train/validation split was stratified to ensure balanced class distributions across splits, improving the model's generalization.\n",
        "\n",
        "For tokenization, the MiniLM tokenizer was used for efficient input encoding, while the model architecture was upgraded to a stronger bert-base-uncased to better handle the complexity of the reduced label set. Hyperparameters were adjusted with a lower learning rate, increased number of epochs (6 total), warmup steps, gradient clipping, and weight decay to promote more stable and effective training.\n",
        "\n",
        "Evaluation metrics, including accuracy and weighted F1 score, were integrated and computed at the end of training to monitor progress.\n",
        "\n",
        "Performance Improvements:\n",
        "- Eval Loss dropped substantially from ~6.02 to ~2.91, indicating better learning and reduced validation error\n",
        "- Eval Accuracy improved dramatically from ~8.3% to ~42.2%, showing the model now classifies many more examples correctly.\n",
        "- Eval F1 Weighted increased from ~0.03 to ~0.31, reflecting improved handling of class imbalance and class differentiation\n",
        "- Training for 6 epochs helped convergence, while evaluation speed remained reasonable and hardware-dependent\n",
        "\n",
        "Overall, the class reduction, stronger BERT, and tuned hyperparameters collectively enabled the model to learn meaningful representations and improve significantly over previous runs."
      ],
      "metadata": {
        "id": "cIXbLWnCvTnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Push Model #2 results to hub for later use\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "21300195ef644364a24b94454678334a",
            "739358fdb3a3459c8a72bd54a9243630",
            "da9a1b65f2494b4687a2f7af00f8a6b5",
            "8a9ba64c8f754fbf94763037e4a26393",
            "6db2192705324d63adf76dd92614f121",
            "5076861cde4c4cf29e06017b7facf326",
            "2e8d7a25016644c0865ade5de92f84a5",
            "d14c1f5cc2fc4973a613dbfd28244b6f",
            "ef68ef7c189446f68ab068ea3236c38b",
            "f4edc8254e4547baabea9916943da2f7",
            "1d31e386587f4391be1c06e8f28e4090"
          ]
        },
        "id": "0A-Cgdb-u27r",
        "outputId": "e251771e-aed9-4ab9-da70-340a33391e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading...:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21300195ef644364a24b94454678334a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/alocken/topic_modeling_project4/commit/4af6ed2c8a434c49d24da312d8fe7052e14d0042', commit_message='End of training', commit_description='', oid='4af6ed2c8a434c49d24da312d8fe7052e14d0042', pr_url=None, repo_url=RepoUrl('https://huggingface.co/alocken/topic_modeling_project4', endpoint='https://huggingface.co', repo_type='model', repo_id='alocken/topic_modeling_project4'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "#New unseen texts to classify\n",
        "data = [\n",
        "    \"The Hubble Space Telescope captured a new image of a galaxy.\",\n",
        "    \"I need help setting up my Linux dual boot system.\",\n",
        "    \"The new graphics card performs exceptionally in gaming benchmarks.\",\n",
        "    \"The Lakers won their game last night against the Bulls.\"\n",
        "]\n",
        "\n",
        "#Load the text-classification pipeline with your Model #2 from the Hub\n",
        "classifier = pipeline(\"text-classification\", model=\"alocken/topic_modeling_project4\")\n",
        "\n",
        "#Run predictions\n",
        "predictions = classifier(data)\n",
        "\n",
        "#Print results\n",
        "for text, pred in zip(data, predictions):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted Label: {pred['label']}, Confidence: {pred['score']:.2f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435,
          "referenced_widgets": [
            "1e7347e767aa480b83d53d9503903c70",
            "9018c8d864b54db583f8eb122310ff90",
            "5fd9c946eb484fd0b33d68034ed94cfa",
            "c2dac11b0860448d85c1255871f31678",
            "4da272bca6844576a1251e8ffadc80aa",
            "ba05eb0410c34761a8e72ade56aa2bc8",
            "2222a23c57a44903a6dcbaa49cea5868",
            "8b3660159f6643758e078f522781a71e",
            "7c64a6e9713e4fb89de3e455f9f1adbe",
            "84da9132a1bc44499f35c542f2d7fe7b",
            "884fb53f17a54327a4d5169f49290ef2",
            "4f5ee6728d45484ea8d5baeae368f1dd",
            "27b157d2c2cb43578e5e4fac0f09a08a",
            "bd21cb0f0f2b4ecfab4b9db0d4cb9d3a",
            "349fbd9e25c944738bcd4d2d18158c9f",
            "8c0d07fe8d2942f8b52041edabfd4afa",
            "0def60ca2aee4e8cba0c8400298d20f3",
            "bf4b0fce6a104b6f949320fc7e0dbbbb",
            "4e461c715e2d44dd8edc6c9350fcf2e8",
            "0b844bfd80b643d2a522a8abc8182841",
            "f84055eabaca43b3af78f45c0eb55764",
            "41dfaf99559d41e5a60994b83730f490",
            "49eac50166284254833458b03f814135",
            "d64ab6f34ca44b219b28d3b08010163a",
            "fb75be5a355945cca4adb2d987216500",
            "c91e67f1f422484aacbcd9c2c7a3f321",
            "ad15c4e0fb464654a63cda1668c44262",
            "97bc776b9eb6463da827f11c465cc611",
            "687181fdddcb498686c4f6fd98698ffe",
            "bd9eb921d9994388aa6ba8286eae1011",
            "ffae181d470641c388f64004e974224e",
            "f8f95996cd52439c91482deb6e971e44",
            "bf37eb22b8674d6d94ca2c04bf45fb00",
            "22159a7954a640faa6653afb25447ce3",
            "c07daee4f26e4f4cb0e09effcdd7dfe0",
            "8d0aaf068f804455b0fe923926ddcdd8",
            "079f36a3603149878b0c305d680e08ce",
            "b46099de383d4c57a947224e4d2912d7",
            "583d689989344e00954385708c3c908c",
            "d6f74bf44d704bce87ce88f0fb91ba14",
            "2dc0ff7a93fb47dd864cdaec12276d83",
            "48aa21dc81a142478907945ee50bf0fd",
            "c2da85db81454018ac6477bdf95d172b",
            "1e9ebed575f744518210607bfbb02da7",
            "ef55596dbccb4fa5a047665c5ea8b8de",
            "3bb3e505f7694e44a2709fc51d8a9afe",
            "e0f0f85d5ad142b68bf783666e07b9a6",
            "eba24a52f1b04f8f9055b022bd12eb0e",
            "2cea04d3ddb8438596bd96154f93e919",
            "b950c734ac50418c95df261327b9512b",
            "01eb948c73444e2bb734236a1500baac",
            "de7fc3f579164e118ebb25cff289a0da",
            "52cf4935f564407ca5bd00d8847fd9d5",
            "51f7cd2fb0c94b81949654eef408d457",
            "90c8efcad8f64a71a3742fa133daf5ff",
            "f2b48760171b48c08c045ea0ebe799c3",
            "90b3fc95120a4b56829233314f32e950",
            "1cd932940ef44dd09d2db030f77f671a",
            "d6b2c1d3c45847039f570bdbe31986a7",
            "7400bee8e4bc4483b7fac3639dc8e420",
            "0d3b1216108d453686719de7e99f95fc",
            "1f77d3893fa74b398432d1cc09d4ee66",
            "31b063954c8346a19ac0ffe2c5744d60",
            "08a582a27ddd44378a11ed6c5f3e41e9",
            "f2e32aa4c97e46cd92e868812e9cb39f",
            "2f87ffc74b094cfa869107a7fb8a062b"
          ]
        },
        "id": "l4bWsg8Qu-LT",
        "outputId": "2dc498d1-19fe-4b25-bf7a-0cac5e1e05ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e7347e767aa480b83d53d9503903c70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f5ee6728d45484ea8d5baeae368f1dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49eac50166284254833458b03f814135"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22159a7954a640faa6653afb25447ce3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef55596dbccb4fa5a047665c5ea8b8de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2b48760171b48c08c045ea0ebe799c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: The Hubble Space Telescope captured a new image of a galaxy.\n",
            "Predicted Label: LABEL_78, Confidence: 0.25\n",
            "\n",
            "Text: I need help setting up my Linux dual boot system.\n",
            "Predicted Label: LABEL_144, Confidence: 0.08\n",
            "\n",
            "Text: The new graphics card performs exceptionally in gaming benchmarks.\n",
            "Predicted Label: LABEL_144, Confidence: 0.23\n",
            "\n",
            "Text: The Lakers won their game last night against the Bulls.\n",
            "Predicted Label: LABEL_119, Confidence: 0.19\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation of model performance on unseen data:**\n",
        "\n",
        "The model's predictions on unseen data show moderate confidence levels ranging roughly from 8% to 25%, indicating some uncertainty in classifying these examples. Each input was assigned a distinct label (e.g., LABEL_78, LABEL_144, LABEL_119), but the relatively low confidence scores suggest the model is not strongly certain about its predictions. This may be due to the complexity of the topic space, overlap between classes, or limited training data for certain topics. While the model can produce plausible categorical outputs, confidence values imply room for improvement for the model to better handle unseen, diverse texts.\n",
        "\n",
        "Ideas to further improve performance:\n",
        "- refining and consolidating topic labels to reduce ambiguity and improve label quality\n",
        "- gathering more balanced training data or using data augmentation to help model learn better across all classes\n",
        "- experimenting with more powerful transformer models or fine-tuning hyperparameters to capture complex patterns and improve overall accuracy"
      ],
      "metadata": {
        "id": "ozuF9IBlvsZt"
      }
    }
  ]
}
